import streamlit as st
import google.generativeai as genai

# [ë§ˆë²•ì˜ ì½”ë“œ]
# 1. ë¡œì»¬ì—ì„œ ì‹¤í–‰í•  ë•? -> .streamlit/secrets.toml íŒŒì¼ì„ ì°¾ì•„ì„œ ì½ìŒ (ì„±ê³µ!)
# 2. í´ë¼ìš°ë“œì—ì„œ ì‹¤í–‰í•  ë•? -> ì›¹ì‚¬ì´íŠ¸ì— ì…ë ¥í•œ Secretsë¥¼ ì°¾ì•„ì„œ ì½ìŒ (ì„±ê³µ!)
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
except FileNotFoundError:
    st.error("ë¹„ë°€ë²ˆí˜¸ íŒŒì¼(secrets.toml)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=api_key)
# ... ì´í›„ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ...
# ---------------------------------------------------------
# [í•„ìˆ˜] ë§¥ë¶ í•œê¸€ ì—ëŸ¬ ë°©ì§€ (ë§¨ ìœ„ì— ìœ ì§€)
import os
import sys
import io

os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["LANG"] = "en_US.UTF-8"
os.environ["LC_ALL"] = "en_US.UTF-8"

if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')
# ---------------------------------------------------------

import streamlit as st
import pandas as pd
from datetime import datetime
import google.generativeai as genai

# ---------------------------------------------------------
# ğŸ”‘ [ì¤‘ìš”] ì—¬ê¸°ì— êµ¬ê¸€ Gemini API í‚¤ë¥¼ ë„£ìœ¼ì„¸ìš”!
api_key = st.secrets["GOOGLE_API_KEY"]
# ---------------------------------------------------------

st.set_page_config(layout="wide", page_title="Super AI Agent (Auto)")

# --- 1. ëª¨ë¸ ìë™ íƒì§€ ë¡œì§ (í•µì‹¬) ---
@st.cache_resource
def get_gemini_model(api_key):
    try:
        genai.configure(api_key=api_key)
        # êµ¬ê¸€ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ìš”ì²­í•©ë‹ˆë‹¤.
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        
        # 1ìˆœìœ„: flash (ë¹ ë¦„), 2ìˆœìœ„: pro (ë˜‘ë˜‘í•¨), 3ìˆœìœ„: ì•„ë¬´ê±°ë‚˜
        target_model_name = None
        for name in available_models:
            if 'flash' in name:
                target_model_name = name
                break
        
        if not target_model_name:
            for name in available_models:
                if 'gemini' in name:
                    target_model_name = name
                    break
        
        if not target_model_name:
            return None, "ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        model = genai.GenerativeModel(target_model_name)
        return model, target_model_name
        
    except Exception as e:
        return None, f"API í‚¤ ì—ëŸ¬ ë˜ëŠ” ì—°ê²° ì‹¤íŒ¨: {str(e)}"

# ëª¨ë¸ ì—°ê²° ì‹œë„
model, model_info = get_gemini_model(api_key)

# --- 2. AI ì§ˆë¬¸ í•¨ìˆ˜ ---
def ask_gemini(user_text):
    if not model:
        return "ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨", "ğŸ”´ ì—ëŸ¬", "ì‹œìŠ¤í…œ", datetime.now().strftime("%H:%M:%S")

    current_time = datetime.now().strftime("%H:%M:%S")
    
    system_instruction = """
    [System Instruction]
    ë„ˆëŠ” 'ì´ˆì¤‘ê³  í•™ìŠµ ì§‘ì¤‘ ë„ìš°ë¯¸ AI'ì•¼.
    1. ê³µë¶€ ì§ˆë¬¸ -> ì†Œí¬ë¼í…ŒìŠ¤ì‹ ì§ˆë¬¸ [STATUS:ğŸŸ¢ í•™ìŠµ ëª°ì… ì¤‘] [CATEGORY:í•™ìŠµ ì§ˆë¬¸]
    2. ë”´ì§“ -> ë‹¨í˜¸í•˜ê²Œ ê±°ì ˆ [STATUS:ğŸ”´ ì§‘ì¤‘ ì´íƒˆ ê²½ê³ ] [CATEGORY:ë”´ì§“/ì´íƒˆ]
    3. ì¸ì‚¬ -> ê³µë¶€ ìœ ë„ [STATUS:ğŸŸ¡ ì¼ë°˜ ëŒ€í™”] [CATEGORY:ì¼ë°˜]
    ë‹µë³€ ëì— [STATUS:...] [CATEGORY:...] íƒœê·¸ë¥¼ ê¼­ ë¶™ì—¬ì¤˜.
    
    [User Question]
    """
    final_prompt = system_instruction + user_text

    try:
        response = model.generate_content(final_prompt)
        full_reply = response.text
        
        if "[STATUS:" in full_reply:
            parts = full_reply.split("[STATUS:")
            ai_reply = parts[0].strip()
            tags = parts[1]
            status = tags.split("]")[0].strip()
            category = tags.split("[CATEGORY:")[1].split("]")[0].strip() if "[CATEGORY:" in tags else "ê¸°íƒ€"
        else:
            ai_reply, status, category = full_reply, "ğŸŸ¡ ì¼ë°˜ ëŒ€í™”", "ì¼ë°˜"
            
        return ai_reply, status, category, current_time
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì—ëŸ¬: {str(e)}", "ğŸ”´ ì—ëŸ¬", "ì‹œìŠ¤í…œ ì—ëŸ¬", current_time

# --- 3. UI êµ¬ì„± ---
if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'focus_score' not in st.session_state: st.session_state.focus_score = 50

st.title("ğŸ« Super AI Agent : ìë™ ëª¨ë¸ ì—°ê²°")

# ìƒë‹¨ì— ì—°ê²°ëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
if model:
    st.success(f"âœ… AI ëª¨ë¸ ì—°ê²° ì„±ê³µ! (ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: `{model_info}`)")
else:
    st.error(f"âŒ ì—°ê²° ì‹¤íŒ¨: {model_info}")

col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ§‘â€ğŸ“ í•™ìƒ í™”ë©´")
    for chat in st.session_state.chat_history:
        with st.chat_message("user"): st.write(chat['user'])
        with st.chat_message("assistant"): st.write(chat['ai'])

    if user_input := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”..."):
        with st.chat_message("user"): st.write(user_input)
        with st.chat_message("assistant"):
            with st.spinner("AI ì„ ìƒë‹˜ ìƒê° ì¤‘..."):
                reply, status, category, time_stamp = ask_gemini(user_input)
                st.write(reply)
        
        score_chg = 5 if "í•™ìŠµ" in status else (-10 if "ì´íƒˆ" in status else 0)
        st.session_state.focus_score = max(0, min(100, st.session_state.focus_score + score_chg))
        st.session_state.chat_history.append({'time': time_stamp, 'user': user_input, 'ai': reply, 'status': status, 'category': category})
        st.rerun()

with col2:
    st.header("ğŸ‘€ í•™ë¶€ëª¨ ìƒí™©ì‹¤")
    score = st.session_state.focus_score
    st.metric("í˜„ì¬ ì§‘ì¤‘ë„", f"{score}ì ")
    st.progress(score / 100)
    
    if st.session_state.chat_history:
        log = st.session_state.chat_history[-1]
        if "ì´íƒˆ" in log['status']: st.error(f"ìƒíƒœ: {log['status']}")
        elif "í•™ìŠµ" in log['status']: st.success(f"ìƒíƒœ: {log['status']}")
        else: st.info(f"ìƒíƒœ: {log['status']}")
        
        st.write(f"**ìµœê·¼ í™œë™:** {log['user']}")
        st.divider()
        st.dataframe(pd.DataFrame(st.session_state.chat_history)[['time', 'category', 'status', 'user']], use_container_width=True)